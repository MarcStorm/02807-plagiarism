\section{Discussion}
% Argue why you have chosen this solution over other solutions
% What is good about your solution/what could be improved


% Talk about what splitting the document into smaller pieces does
% - Allow to match against multiple documents and not just the similarity against a single one.
% - The space usage of the data structure grows proportionally with the amount of paragraphs

% If the document/paragraph is too short, then the program breaks because of the shingles. Mention

\paragraph{Database}
% Lookup in a database is slower than having datastructure in RAM.
We have made the choice of using a database instead of storing the LSH data structure in memory which has some trade-offs. Accessing data from disk will be much slower than accessing it from memory and thus will increase lookup times. Asymptotically however there is no difference because we still have to perform $b$ lookups, one for each band. The performance decrease can be tolerated since the lookup time is not significantly important to the speed of the algorithm as a whole, and in practice only accounts for a small fraction of the total execution time. In fact, generating shingles and computing minhashes is the single most intensive task of finding candidates.

% Mention that in reality you should use a production database
% Mention limitations of SQLite, i.e. crash when many threads working on it.
During the implementation of the solution we found that generating signatures and writing them to the SQLite database with multiple threads resulted in unexpected behaviour. Having 8 threads constantly writing signatures to the database caused it to fail because of a time out, hence the SQLite data base is simply not wired for that level of concurrency. For this reason one should consider to use a client/server database engine instead, such as PostgreSQL or MySQL, as that will be able to support the higher level of concurrency.\footnote{\url{https://www.sqlite.org/faq.html\#q5}}. Using a proper production data base might also help reduce the massive overhead of storing the data.

\subsection{Alternative solutions}
% MATHIAS insert whatever the peergrade preliminary report were using 
Using LSH is a good approach to perform a nearest-neighbour search of the Wikipedia articles with fast lookup time and low memory usage. However one may consider alternative solutions to the problem of searching neighbours. A solution would be representing documents as vectors and performing clustering on said vectors. This could be achieved through a clustering algorithm such as K-means clustering. Clustering our data would enable us to quickly eliminate the search space and bloom filters could afterwards be used to perform a membership test of the cluster (with some false positives). How we would chose to represent our articles in a vector would require some ingenuity. Using LSH we have a straightforward and noble approach as to how we represent our data and perform the lookup.

% Are you comparing a given document to every single article in the wiki database?

% Future work / what could be improved
\subsection{Future work}
\paragraph{Citations}
Detecting that something was plagiarised from a Wikipedia article with the current implementation, the solution is unable to check whether it was cited or not. If the text has been cited correctly it should ideally not be marked as plagiarised. For further improvements of the application one could consider to implement support for this. As well as being able to cross check that the cited text is actually coming from the reference. Even though there are guidelines for citing, it could vary a lot from document to document, so a robust method would have to be set up, to find the varying citations. Furthermore the cross checking will rely on how the reference was entered in the document itself, in order for the tool to correctly find it. Should one decide to implement the cross checker, it would either require the reference to be a Wikipedia article or require the database to be extended with more than just Wikipedia articles.

\paragraph{Highlighting plagiarised content}
The current implementation will only present the user with a list of candidates where the content of the query document could be plagiarised from. To improve the usability for the user, it could be considered to implement a visual representation of where in the candidate document the paragraph was considered plagiarised from. This could possibly be implemented by using \emph{difflib}\footnote{\url{https://docs.python.org/2/library/difflib.html}} amongst other tools.

\paragraph{Filtration of candidates}
% Explain how candidates could be filtered based on a similarity.
% The is nothing covering how candidates are tested for actual similarity to filter out the false positives.
When the candidates are found by a lookup, it could be that false positives were added to the list. With the current values of $b$ and $r$ for LSH and given a Jaccard similarity of for example 0.20 between a paragraph of the query document and a paragraph in the database, then the probability of that document being marked as a candidate is 0.64\%. Having an additional layer for the lookup to validate the Jaccard similarity of the candidates could filter these false positives out. However, this would require the current implementation to be extended, to find the exact paragraph that triggered the document to be marked as a candidate.